{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f805463",
   "metadata": {},
   "source": [
    "# Assignment 03 - AE21B105\n",
    "This notebook will be used as the base version of testing the code written and for the sweeps later on. Then once finalized and all good this will be transfered to a script with command line arguments. Lets begin !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94cb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as Fn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(linewidth=50)\n",
    "np.set_printoptions(linewidth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b935a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      native    latin  count\n",
      "0     ஃபியட்     fiat      2\n",
      "1     ஃபியட்   phiyat      1\n",
      "2     ஃபியட்    piyat      1\n",
      "3  ஃபிரான்ஸ்  firaans      1\n",
      "4  ஃபிரான்ஸ்   france      2\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Loading the dataset\n",
    "df_train = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.train.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_test = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.test.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_val = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.dev.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "\n",
    "\n",
    "# Show first few rows\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset for the Seq2Seq model\n",
    "class Dataset_Tamil(Dataset):\n",
    "    def __init__(self, dataframe, build_vocab=True, input_token_index=None, output_token_index=None,\n",
    "                 max_enc_seq_len=0, max_dec_seq_len=0):\n",
    "        \n",
    "        # Input variables\n",
    "        self.input_df = dataframe\n",
    "        self.input_words = []\n",
    "        self.output_words = []\n",
    "        # Characters of the language\n",
    "        self.input_characters = set()\n",
    "        self.output_characters = set()\n",
    "\n",
    "        # Iterating thorough the rows\n",
    "        for _, row in self.input_df.iterrows():\n",
    "            input_word = str(row[\"latin\"])\n",
    "            output_word = \"\\t\" + str(row[\"native\"]) + \"\\n\"\n",
    "            self.input_words.append(input_word)\n",
    "            self.output_words.append(output_word)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.build_vocab()\n",
    "        else:\n",
    "            # Token index for sequence building\n",
    "            self.input_token_index = input_token_index\n",
    "            self.output_token_index = output_token_index\n",
    "            # Heuristics lengths for the encoder decoder\n",
    "            self.max_enc_seq_len = max_enc_seq_len\n",
    "            self.max_dec_seq_len = max_dec_seq_len\n",
    "\n",
    "        # Finding the encoder/decoder tokens \n",
    "        self.total_encoder_tokens = len(self.input_token_index)\n",
    "        self.total_decoder_tokens = len(self.output_token_index)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Building the vocabulary\n",
    "        self.input_characters = sorted(set(\" \".join(self.input_words)))\n",
    "        self.output_characters = sorted(set(\" \".join(self.output_words)))\n",
    "        # Adding the padding character if not present\n",
    "        if \" \" not in self.input_characters:\n",
    "            self.input_characters.append(\" \")\n",
    "        if \" \" not in self.output_characters:\n",
    "            self.output_characters.append(\" \")\n",
    "\n",
    "        # Fitting/Finding the necessary values from training data\n",
    "        self.input_token_index = {char: i for i, char in enumerate(self.input_characters)}\n",
    "        self.output_token_index = {char: i for i, char in enumerate(self.output_characters)}\n",
    "\n",
    "        self.max_enc_seq_len = max(len(txt) for txt in self.input_words)\n",
    "        self.max_dec_seq_len = max(len(txt) for txt in self.output_words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_words)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_word = self.input_words[index]\n",
    "        output_word = self.output_words[index]\n",
    "\n",
    "        # Finding the input for each stages of the network\n",
    "        encoder_input = np.zeros((self.max_enc_seq_len, self.total_encoder_tokens), dtype=np.float32)\n",
    "        decoder_input = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "        decoder_output = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "\n",
    "        for t, char in enumerate(input_word):\n",
    "            if char in self.input_token_index:\n",
    "                encoder_input[t, self.input_token_index[char]] = 1.0\n",
    "        for t in range(len(input_word), self.max_enc_seq_len):\n",
    "            encoder_input[t, self.input_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t, char in enumerate(output_word):\n",
    "            if char in self.output_token_index:\n",
    "                decoder_input[t, self.output_token_index[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    decoder_output[t - 1, self.output_token_index[char]] = 1.0\n",
    "        # Fill remaining positions with space character\n",
    "        for t in range(len(output_word), self.max_dec_seq_len):\n",
    "            decoder_input[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t in range(len(output_word) - 1, self.max_dec_seq_len):\n",
    "            decoder_output[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(encoder_input),\n",
    "            torch.from_numpy(decoder_input),\n",
    "            torch.from_numpy(decoder_output)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acf2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and dataloaders\n",
    "train_dataset = Dataset_Tamil(df_train)\n",
    "val_dataset = Dataset_Tamil(df_val, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "test_dataset = Dataset_Tamil(df_test, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes definitions #\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type=\"RNN\", num_layers=1, bi_directional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.enc = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.enc = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "        else:\n",
    "            self.enc = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden, (hn, cn) = self.enc(x)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.enc(x)\n",
    "            return hidden, out\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type='RNN', num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.dec = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.dec = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        else:\n",
    "            self.dec = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if type(states) == tuple:\n",
    "            hidden, (hn, cn) = self.dec(x, states)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.dec(x, states)\n",
    "            return hidden, out\n",
    "\n",
    "def combine_directions(hidden):\n",
    "    layers = []\n",
    "    for i in range(0, hidden.size(0), 2): \n",
    "        fwd = hidden[i]\n",
    "        bwd = hidden[i + 1]\n",
    "        combined = torch.cat((fwd, bwd), dim=-1)  \n",
    "        layers.append(combined)\n",
    "    return torch.stack(layers)  \n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_token_index, output_token_index, max_dec_seq_len, embedding_dim,hidden_size_enc, bi_directional,\n",
    "            nature=\"train\", enc_cell=\"LSTM\", dec_cell=\"LSTM\", num_layers=1,dropout=0.2, device=\"cpu\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_index_token = input_token_index\n",
    "        self.output_index_token = output_token_index\n",
    "        self.max_dec_seq_len = max_dec_seq_len\n",
    "        self.nature = nature\n",
    "        self.enc_cell_type = enc_cell.upper()\n",
    "        self.dec_cell_type = dec_cell.upper()\n",
    "        self.num_layers= num_layers\n",
    "        self.bi_directional = bi_directional\n",
    "        self.hidden_size_enc = hidden_size_enc\n",
    "        self.hidden_size_dec = (1 + int(self.bi_directional == True))*hidden_size_enc\n",
    "        self.embedding = nn.Linear(in_features=len(self.input_index_token), out_features=embedding_dim)\n",
    "        self.embedding_act = nn.Tanh()\n",
    "        self.encoder = Encoder(input_size=embedding_dim, hidden_size=hidden_size_enc, dropout=dropout, cell_type=enc_cell, num_layers=num_layers, bi_directional=self.bi_directional).to(device)\n",
    "        self.decoder = Decoder(input_size=len(self.output_index_token), hidden_size=self.hidden_size_dec, dropout=dropout, cell_type=dec_cell, num_layers=num_layers).to(device)\n",
    "        self.device = device\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size_dec, out_features=len(output_token_index))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        if self.bi_directional == True:\n",
    "            if self.enc_cell_type == \"LSTM\":\n",
    "                (h,c) = states_enc\n",
    "                states_enc = (combine_directions(h), combine_directions(c))\n",
    "            else:\n",
    "                states_enc = combine_directions(states_enc)\n",
    "\n",
    "        # Teacher forcing mode #    \n",
    "        # Making the states correctly formatted\n",
    "        if self.dec_cell_type == \"LSTM\": \n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc\n",
    "            else:\n",
    "                h = torch.zeros(self.num_layers, batch_size, self.decoder.hidden_size, device=self.device)\n",
    "                c = states_enc\n",
    "                states_dec = (h, c)\n",
    "        else:\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc[1]\n",
    "            else:\n",
    "                states_dec = states_enc\n",
    "\n",
    "        # Decoder gives the outputs batchwise\n",
    "        decoder_outputs, _ = self.decoder(DEC_IN, states_dec)  \n",
    "        logits = self.fc(decoder_outputs)                      \n",
    "        return logits\n",
    "\n",
    "    def predict_greedy(self, batch):\n",
    "        # Greedy force outputs #\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        if self.bi_directional == True:\n",
    "            if self.enc_cell_type == \"LSTM\":\n",
    "                (h,c) = states_enc\n",
    "                states_enc = (combine_directions(h), combine_directions(c))\n",
    "            else:\n",
    "                states_enc = combine_directions(states_enc)\n",
    "            \n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, self.max_dec_seq_len, len(self.output_index_token), device=self.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "        in_[:, 0, 0] = 1.0\n",
    "        # Making the states correctly formatted\n",
    "        if self.dec_cell_type == \"LSTM\":\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc\n",
    "            else:\n",
    "                h = torch.zeros(self.num_layers, batch_size, self.decoder.hidden_size, device=self.device)\n",
    "                c = states_enc\n",
    "                states_dec = (h, c)\n",
    "        else:\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc[1]\n",
    "\n",
    "        # Output to input\n",
    "        for t in range(self.max_dec_seq_len):\n",
    "            out_step, states_dec = self.decoder(in_, states_dec) \n",
    "            logits_step = self.fc(out_step.squeeze(1))            \n",
    "            final_out[:, t, :] = logits_step\n",
    "\n",
    "            # Greedy argmax for next input\n",
    "            top1 = torch.argmax(logits_step, dim=1)               \n",
    "            in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "            in_[torch.arange(batch_size), 0, top1] = 1.0\n",
    "\n",
    "        return final_out\n",
    "    \n",
    "    def predict_beam_search(self, batch, beam_width=3):\n",
    "        ENC_IN, _, _ = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "\n",
    "        # Encoder forward pass #\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        # Final output tensor to hold decoded logits\n",
    "        final_out = torch.zeros(batch_size, self.max_dec_seq_len, len(self.output_index_token), device=self.device)\n",
    "\n",
    "        if self.bi_directional == True:\n",
    "            if self.enc_cell_type == \"LSTM\":\n",
    "                (h,c) = states_enc\n",
    "                states_enc = (combine_directions(h), combine_directions(c))\n",
    "            else:\n",
    "                states_enc = combine_directions(states_enc)\n",
    "\n",
    "        # Format encoder states for decoder\n",
    "        if self.dec_cell_type == \"LSTM\":\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc\n",
    "            else:\n",
    "                h = torch.zeros(self.num_layers, batch_size, self.decoder.hidden_size, device=self.device)\n",
    "                c = states_enc\n",
    "                states_dec = (h, c)\n",
    "        else:\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc[1]\n",
    "            else:\n",
    "                states_dec = states_enc\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            # Initial decoder input\n",
    "            in_ = torch.zeros(beam_width, 1, len(self.output_index_token), device=self.device)\n",
    "            in_[:, 0, 0] = 1.0\n",
    "\n",
    "            # Initialize beams\n",
    "            beam_dict_old = {x: {\"sequence\": [0], \"states\": None, \"log_probs\": 0} for x in range(beam_width)}\n",
    "            beam_dict_new = {x: {\"sequence\": [], \"states\": None, \"log_probs\": float('-inf')} for x in range(beam_width)}\n",
    "\n",
    "            if self.dec_cell_type == \"LSTM\":\n",
    "                h_enc, c_enc = states_dec\n",
    "                for ix in range(beam_width):\n",
    "                    beam_dict_old[ix][\"states\"] = (\n",
    "                        h_enc[:, batch_idx:batch_idx+1, :].contiguous(),  \n",
    "                        c_enc[:, batch_idx:batch_idx+1, :].contiguous()\n",
    "                    )\n",
    "            else:\n",
    "                for ix in range(beam_width):\n",
    "                    beam_dict_old[ix][\"states\"] = states_dec[:, batch_idx:batch_idx+1, :].contiguous()\n",
    "\n",
    "\n",
    "            for t in range(self.max_dec_seq_len):\n",
    "                # Prepare decoder input and hidden states\n",
    "                in_ = torch.zeros(beam_width, 1, len(self.output_index_token), device=self.device)\n",
    "                states_h_tensor = torch.zeros((self.num_layers, beam_width, self.hidden_size_enc), device=self.device)\n",
    "                states_c_tensor = torch.zeros((self.num_layers, beam_width, self.hidden_size_enc), device=self.device)\n",
    "\n",
    "                for ix in range(beam_width):\n",
    "                    token_idx = beam_dict_old[ix][\"sequence\"][-1]\n",
    "                    in_[ix, 0, token_idx] = 1\n",
    "                    if self.dec_cell_type == \"LSTM\":\n",
    "                        states_h_tensor[:, ix, :] = beam_dict_old[ix][\"states\"][0].squeeze(1)\n",
    "                        states_c_tensor[:, ix, :] = beam_dict_old[ix][\"states\"][1].squeeze(1)\n",
    "                    else:\n",
    "                        states_h_tensor[:, ix, :] = beam_dict_old[ix][\"states\"].squeeze(1)\n",
    "\n",
    "                states_dec_i = (states_h_tensor, states_c_tensor) if self.dec_cell_type == \"LSTM\" else states_h_tensor\n",
    "\n",
    "                out_step, states_dec_out = self.decoder(in_, states_dec_i)\n",
    "                logits_step = self.fc(out_step.squeeze(1))\n",
    "                log_prob = F.log_softmax(logits_step, dim=1)\n",
    "\n",
    "                topk_log_probs, topk_indices = torch.topk(log_prob, k=beam_width, dim=1)\n",
    "\n",
    "                all_candidates = []\n",
    "                for i in range(beam_width):\n",
    "                    for j in range(beam_width):\n",
    "                        new_seq = beam_dict_old[i][\"sequence\"] + [topk_indices[i, j].item()]\n",
    "                        new_log_prob = beam_dict_old[i][\"log_probs\"] + topk_log_probs[i, j].item()\n",
    "\n",
    "                        if self.dec_cell_type == \"LSTM\":\n",
    "                            new_state = (\n",
    "                                states_dec_out[0][:, i:i+1, :].clone(),\n",
    "                                states_dec_out[1][:, i:i+1, :].clone()\n",
    "                            )\n",
    "                        else:\n",
    "                            new_state = states_dec_out[:, i:i+1, :].clone()\n",
    "\n",
    "                        all_candidates.append({\n",
    "                            \"sequence\": new_seq,\n",
    "                            \"states\": new_state,\n",
    "                            \"log_probs\": new_log_prob\n",
    "                        })\n",
    "\n",
    "                # Selecting top beam_width sequences\n",
    "                all_candidates = sorted(all_candidates, key=lambda x: x[\"log_probs\"], reverse=True)\n",
    "                for ix in range(beam_width):\n",
    "                    beam_dict_old[ix] = all_candidates[ix]\n",
    "\n",
    "            # Choosing the best final sequence for this sample\n",
    "            best_sequence = beam_dict_old[0][\"sequence\"]\n",
    "            for t, token in enumerate(best_sequence[1:]):  \n",
    "                final_out[batch_idx, t, token] = 1.0\n",
    "\n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)  # 2 is the padding index\n",
    "\n",
    "    print(\"Training of the model has started...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, desc=f\"Epoch : {epoch + 1} \", ncols=100)\n",
    "\n",
    "        for batch in tqdm_loader:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "            # Move to device\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Reshape for loss\n",
    "            decoder_output = decoder_output.view(-1, decoder_output.size(-1))\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            tqdm_loader.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_acc, val_word_acc = validate_seq2seq(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Word Acc: {val_word_acc:.4f}\")\n",
    "\n",
    "def validate_seq2seq(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_words = 0\n",
    "    total_words = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            vocab_size = decoder_output.size(-1)\n",
    "            decoder_output = decoder_output.view(-1, vocab_size)\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Character-wise accuracy\n",
    "            decoder_output = model.predict_greedy(batch)\n",
    "\n",
    "            #print(decoder_output.shape)\n",
    "            pred_tokens = decoder_output.argmax(dim=2)\n",
    "            true_tokens = DEC_OUT.argmax(dim=2)\n",
    "            #print(pred_tokens.shape)\n",
    "            \n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            correct_chars += (pred_tokens[mask] == true_tokens[mask]).sum().item()\n",
    "            total_chars += mask.sum().item()\n",
    "\n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            #print(mask.shape)\n",
    "            total_words += decoder_output.shape[0]\n",
    "            #print(pred_tokens[mask].shape)\n",
    "            chk_words = (mask.int() - (pred_tokens == true_tokens).int())\n",
    "            chk_words[mask == False] = 0\n",
    "            correct_words += (chk_words.sum(dim = 1) == 0).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_chars / total_chars if total_chars > 0 else 0.0\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0.0\n",
    "    return avg_loss, accuracy, word_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7483e27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5897109873383959, 0.6492114165912232, 0.26966456715980663)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, train_dataset.max_dec_seq_len,embedding_dim=50,hidden_size_enc=64, hidden_size_dec=64, num_layers=2, device=device).to(device)\n",
    "validate_seq2seq(model=model,val_loader=val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3b9f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1 : 100%|█████████████████████████████| 1066/1066 [00:53<00:00, 19.80it/s, Train Loss=0.852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 1.6703\n",
      "Epoch [1/20] | Val Loss: 0.9205 | Val Acc: 0.4911 | Val Word Acc: 0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2 : 100%|█████████████████████████████| 1066/1066 [00:55<00:00, 19.20it/s, Train Loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] | Train Loss: 0.7117\n",
      "Epoch [2/20] | Val Loss: 0.5844 | Val Acc: 0.6543 | Val Word Acc: 0.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3 :   2%|▌                              | 19/1066 [00:01<00:55, 18.72it/s, Train Loss=0.519]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, train_dataset.max_dec_seq_len,bi_directional=\u001b[38;5;28;01mTrue\u001b[39;00m,embedding_dim=\u001b[32m50\u001b[39m,hidden_size_enc=\u001b[32m64\u001b[39m,enc_cell=\u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m, dec_cell=\u001b[33m\"\u001b[39m\u001b[33mGRU\u001b[39m\u001b[33m\"\u001b[39m, num_layers=\u001b[32m2\u001b[39m, device=device).to(device)\n\u001b[32m      3\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_seq2seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_seq2seq\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m     15\u001b[39m DEC_OUT = DEC_OUT.to(device)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Move to device\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m decoder_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Reshape for loss\u001b[39;00m\n\u001b[32m     20\u001b[39m decoder_output = decoder_output.view(-\u001b[32m1\u001b[39m, decoder_output.size(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mSeq2Seq.forward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    124\u001b[39m         states_dec = states_enc\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Decoder gives the outputs batchwise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m decoder_outputs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEC_IN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates_dec\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, T, H)\u001b[39;00m\n\u001b[32m    128\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.fc(decoder_outputs)                      \u001b[38;5;66;03m# (B, T, Vocab)\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, states)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden, (hn, cn)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     hidden, out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden, out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1393\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1393\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     result = _VF.gru(\n\u001b[32m   1406\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1407\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1414\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1415\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, train_dataset.max_dec_seq_len,bi_directional=True,embedding_dim=50,hidden_size_enc=64,enc_cell=\"LSTM\", dec_cell=\"GRU\", num_layers=2, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs=20, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
