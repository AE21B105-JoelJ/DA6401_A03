{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f805463",
   "metadata": {},
   "source": [
    "# Assignment 03 - AE21B105\n",
    "This notebook will be used as the base version of testing the code written and for the sweeps later on. Then once finalized and all good this will be transfered to a script with command line arguments. Lets begin !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94cb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(linewidth=50)\n",
    "np.set_printoptions(linewidth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b935a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      native    latin  count\n",
      "0     ஃபியட்     fiat      2\n",
      "1     ஃபியட்   phiyat      1\n",
      "2     ஃபியட்    piyat      1\n",
      "3  ஃபிரான்ஸ்  firaans      1\n",
      "4  ஃபிரான்ஸ்   france      2\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Loading the dataset\n",
    "df_train = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.train.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_test = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.test.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_val = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.dev.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "\n",
    "\n",
    "# Show first few rows\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset for the Seq2Seq model\n",
    "class Dataset_Tamil(Dataset):\n",
    "    def __init__(self, dataframe, build_vocab=True, input_token_index=None, output_token_index=None,\n",
    "                 max_enc_seq_len=0, max_dec_seq_len=0):\n",
    "        \n",
    "        # Input variables\n",
    "        self.input_df = dataframe\n",
    "        self.input_words = []\n",
    "        self.output_words = []\n",
    "        # Characters of the language\n",
    "        self.input_characters = set()\n",
    "        self.output_characters = set()\n",
    "\n",
    "        # Iterating thorough the rows\n",
    "        for _, row in self.input_df.iterrows():\n",
    "            input_word = str(row[\"latin\"])\n",
    "            output_word = \"\\t\" + str(row[\"native\"]) + \"\\n\"\n",
    "            self.input_words.append(input_word)\n",
    "            self.output_words.append(output_word)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.build_vocab()\n",
    "        else:\n",
    "            # Token index for sequence building\n",
    "            self.input_token_index = input_token_index\n",
    "            self.output_token_index = output_token_index\n",
    "            # Heuristics lengths for the encoder decoder\n",
    "            self.max_enc_seq_len = max_enc_seq_len\n",
    "            self.max_dec_seq_len = max_dec_seq_len\n",
    "\n",
    "        # Finding the encoder/decoder tokens \n",
    "        self.total_encoder_tokens = len(self.input_token_index)\n",
    "        self.total_decoder_tokens = len(self.output_token_index)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Building the vocabulary\n",
    "        self.input_characters = sorted(set(\" \".join(self.input_words)))\n",
    "        self.output_characters = sorted(set(\" \".join(self.output_words)))\n",
    "        # Adding the padding character if not present\n",
    "        if \" \" not in self.input_characters:\n",
    "            self.input_characters.append(\" \")\n",
    "        if \" \" not in self.output_characters:\n",
    "            self.output_characters.append(\" \")\n",
    "\n",
    "        # Fitting/Finding the necessary values from training data\n",
    "        self.input_token_index = {char: i for i, char in enumerate(self.input_characters)}\n",
    "        self.output_token_index = {char: i for i, char in enumerate(self.output_characters)}\n",
    "\n",
    "        self.max_enc_seq_len = max(len(txt) for txt in self.input_words)\n",
    "        self.max_dec_seq_len = max(len(txt) for txt in self.output_words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_words)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_word = self.input_words[index]\n",
    "        output_word = self.output_words[index]\n",
    "\n",
    "        # Finding the input for each stages of the network\n",
    "        encoder_input = np.zeros((self.max_enc_seq_len, self.total_encoder_tokens), dtype=np.float32)\n",
    "        decoder_input = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "        decoder_output = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "\n",
    "        for t, char in enumerate(input_word):\n",
    "            if char in self.input_token_index:\n",
    "                encoder_input[t, self.input_token_index[char]] = 1.0\n",
    "        for t in range(len(input_word), self.max_enc_seq_len):\n",
    "            encoder_input[t, self.input_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t, char in enumerate(output_word):\n",
    "            if char in self.output_token_index:\n",
    "                decoder_input[t, self.output_token_index[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    decoder_output[t - 1, self.output_token_index[char]] = 1.0\n",
    "        # Fill remaining positions with space character\n",
    "        for t in range(len(output_word), self.max_dec_seq_len):\n",
    "            decoder_input[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        # Ensure decoder_output is padded *after* last real target (t - 1 from above loop)\n",
    "        for t in range(len(output_word) - 1, self.max_dec_seq_len):\n",
    "            decoder_output[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(encoder_input),\n",
    "            torch.from_numpy(decoder_input),\n",
    "            torch.from_numpy(decoder_output)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acf2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and dataloaders\n",
    "train_dataset = Dataset_Tamil(df_train)\n",
    "val_dataset = Dataset_Tamil(df_val, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "test_dataset = Dataset_Tamil(df_test, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type=\"RNN\", num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.enc = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.enc = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        else:\n",
    "            self.enc = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden, (hn, cn) = self.enc(x)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.enc(x)\n",
    "            return hidden, out\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type='RNN', num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.dec = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.dec = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        else:\n",
    "            self.dec = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if type(states) == tuple:\n",
    "            hidden, (hn, cn) = self.dec(x, states)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.dec(x, states)\n",
    "            return hidden, out\n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_token_index, output_token_index, max_dec_seq_len, embedding_dim,hidden_size_enc, hidden_size_dec, nature=\"train\", enc_cell=\"LSTM\", dec_cell=\"LSTM\", num_layers=1, dropout=0.2, device=\"cpu\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_index_token = input_token_index\n",
    "        self.output_index_token = output_token_index\n",
    "        self.max_dec_seq_len = max_dec_seq_len\n",
    "        self.nature = nature\n",
    "        self.enc_cell_type = enc_cell.upper()\n",
    "        self.dec_cell_type = dec_cell.upper()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Linear(in_features=len(self.input_index_token), out_features=embedding_dim)\n",
    "        self.embedding_act = nn.Tanh()\n",
    "        self.encoder = Encoder(input_size=embedding_dim, hidden_size=hidden_size_enc, dropout=dropout, cell_type=enc_cell, num_layers=num_layers).to(device)\n",
    "        self.decoder = Decoder(input_size=len(self.output_index_token), hidden_size=hidden_size_dec, dropout=dropout, cell_type=dec_cell, num_layers=num_layers).to(device)\n",
    "        self.device = device\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.fc = nn.Linear(in_features=hidden_size_dec, out_features=len(output_token_index))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        # Teacher forcing mode #    \n",
    "        # Making the states correctly formatted\n",
    "        if self.dec_cell_type == \"LSTM\": \n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc\n",
    "            else:\n",
    "                h = torch.zeros(self.num_layers, batch_size, self.decoder.hidden_size, device=self.device)\n",
    "                c = states_enc\n",
    "                states_dec = (h, c)\n",
    "        else:\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc[1]\n",
    "\n",
    "        # Decoder gives the outputs batchwise\n",
    "        decoder_outputs, _ = self.decoder(DEC_IN, states_dec)  # (B, T, H)\n",
    "        logits = self.fc(decoder_outputs)                      # (B, T, Vocab)\n",
    "        return logits\n",
    "\n",
    "    def predict_greedy(self, batch):\n",
    "        # Greedy force outputs #\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "            \n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, self.max_dec_seq_len, len(self.output_index_token), device=self.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "        in_[:, 0, 0] = 1.0\n",
    "        # Making the states correctly formatted\n",
    "        if self.dec_cell_type == \"LSTM\":\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc\n",
    "            else:\n",
    "                h = torch.zeros(self.num_layers, batch_size, self.decoder.hidden_size, device=self.device)\n",
    "                c = states_enc\n",
    "                states_dec = (h, c)\n",
    "        else:\n",
    "            if isinstance(states_enc, tuple):\n",
    "                states_dec = states_enc[1]\n",
    "\n",
    "        # Output to input\n",
    "        for t in range(self.max_dec_seq_len):\n",
    "            out_step, states_dec = self.decoder(in_, states_dec)  # (B, 1, H)\n",
    "            logits_step = self.fc(out_step.squeeze(1))            # (B, V)\n",
    "            final_out[:, t, :] = logits_step\n",
    "\n",
    "            # Greedy argmax for next input\n",
    "            top1 = torch.argmax(logits_step, dim=1)               # (B,)\n",
    "            in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "            in_[torch.arange(batch_size), 0, top1] = 1.0\n",
    "\n",
    "        return final_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)  # 2 is the padding index\n",
    "\n",
    "    print(\"Training of the model has started...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, desc=f\"Epoch : {epoch + 1} \", ncols=100)\n",
    "\n",
    "        for batch in tqdm_loader:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "            # Move to device\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Reshape for loss\n",
    "            decoder_output = decoder_output.view(-1, decoder_output.size(-1))\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            tqdm_loader.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_acc, val_word_acc = validate_seq2seq(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Word Acc: {val_word_acc:.4f}\")\n",
    "\n",
    "def validate_seq2seq(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_words = 0\n",
    "    total_words = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            vocab_size = decoder_output.size(-1)\n",
    "            decoder_output = decoder_output.view(-1, vocab_size)\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Character-wise accuracy\n",
    "            decoder_output = model.predict_greedy(batch)\n",
    "            #print(decoder_output.shape)\n",
    "            pred_tokens = decoder_output.argmax(dim=2)#.view(DEC_OUT.size(0), DEC_OUT.size(1))\n",
    "            true_tokens = DEC_OUT.argmax(dim=2)\n",
    "            #print(pred_tokens.shape)\n",
    "            #print(true_tokens.shape)\n",
    "            \n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            correct_chars += (pred_tokens[mask] == true_tokens[mask]).sum().item()\n",
    "            total_chars += mask.sum().item()\n",
    "\n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            #print(mask.shape)\n",
    "            total_words += decoder_output.shape[0]\n",
    "            #print(pred_tokens[mask].shape)\n",
    "            chk_words = (mask.int() - (pred_tokens == true_tokens).int())\n",
    "            chk_words[mask == False] = 0\n",
    "            correct_words += (chk_words.sum(dim = 1) == 0).sum().item()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_chars / total_chars if total_chars > 0 else 0.0\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0.0\n",
    "    return avg_loss, accuracy, word_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f3b9f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1 : 100%|██████████████████████████████| 1066/1066 [00:23<00:00, 44.54it/s, Train Loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 2.3704\n",
      "Epoch [1/20] | Val Loss: 1.9619 | Val Acc: 0.1783 | Val Word Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2 : 100%|██████████████████████████████| 1066/1066 [00:23<00:00, 45.70it/s, Train Loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] | Train Loss: 1.7463\n",
      "Epoch [2/20] | Val Loss: 1.5836 | Val Acc: 0.2464 | Val Word Acc: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3 : 100%|██████████████████████████████| 1066/1066 [00:23<00:00, 45.27it/s, Train Loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] | Train Loss: 1.4094\n",
      "Epoch [3/20] | Val Loss: 1.3270 | Val Acc: 0.3206 | Val Word Acc: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4 : 100%|██████████████████████████████| 1066/1066 [00:24<00:00, 42.76it/s, Train Loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] | Train Loss: 1.1813\n",
      "Epoch [4/20] | Val Loss: 1.1259 | Val Acc: 0.3756 | Val Word Acc: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 5 : 100%|█████████████████████████████| 1066/1066 [00:22<00:00, 47.68it/s, Train Loss=0.798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] | Train Loss: 0.9933\n",
      "Epoch [5/20] | Val Loss: 0.9770 | Val Acc: 0.4355 | Val Word Acc: 0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 6 : 100%|█████████████████████████████| 1066/1066 [00:22<00:00, 48.21it/s, Train Loss=0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] | Train Loss: 0.8460\n",
      "Epoch [6/20] | Val Loss: 0.8440 | Val Acc: 0.5010 | Val Word Acc: 0.1206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 7 : 100%|█████████████████████████████| 1066/1066 [00:22<00:00, 47.12it/s, Train Loss=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] | Train Loss: 0.7387\n",
      "Epoch [7/20] | Val Loss: 0.7651 | Val Acc: 0.5312 | Val Word Acc: 0.1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 8 : 100%|█████████████████████████████| 1066/1066 [00:24<00:00, 42.96it/s, Train Loss=0.546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] | Train Loss: 0.6625\n",
      "Epoch [8/20] | Val Loss: 0.7037 | Val Acc: 0.5688 | Val Word Acc: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 9 : 100%|█████████████████████████████| 1066/1066 [00:23<00:00, 45.04it/s, Train Loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] | Train Loss: 0.6041\n",
      "Epoch [9/20] | Val Loss: 0.6626 | Val Acc: 0.5964 | Val Word Acc: 0.2165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 10 : 100%|████████████████████████████| 1066/1066 [00:24<00:00, 43.46it/s, Train Loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] | Train Loss: 0.5595\n",
      "Epoch [10/20] | Val Loss: 0.6394 | Val Acc: 0.6178 | Val Word Acc: 0.2374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 11 : 100%|████████████████████████████| 1066/1066 [00:24<00:00, 44.21it/s, Train Loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] | Train Loss: 0.5254\n",
      "Epoch [11/20] | Val Loss: 0.6015 | Val Acc: 0.6367 | Val Word Acc: 0.2606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 12 : 100%|████████████████████████████| 1066/1066 [00:24<00:00, 42.68it/s, Train Loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] | Train Loss: 0.4969\n",
      "Epoch [12/20] | Val Loss: 0.5752 | Val Acc: 0.6523 | Val Word Acc: 0.2848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 13 : 100%|████████████████████████████| 1066/1066 [00:25<00:00, 42.61it/s, Train Loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] | Train Loss: 0.4714\n",
      "Epoch [13/20] | Val Loss: 0.5612 | Val Acc: 0.6606 | Val Word Acc: 0.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 14 : 100%|████████████████████████████| 1066/1066 [00:24<00:00, 44.15it/s, Train Loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] | Train Loss: 0.4506\n",
      "Epoch [14/20] | Val Loss: 0.5452 | Val Acc: 0.6787 | Val Word Acc: 0.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 15 : 100%|████████████████████████████| 1066/1066 [00:21<00:00, 50.71it/s, Train Loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] | Train Loss: 0.4316\n",
      "Epoch [15/20] | Val Loss: 0.5313 | Val Acc: 0.6815 | Val Word Acc: 0.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 16 : 100%|████████████████████████████| 1066/1066 [00:20<00:00, 51.18it/s, Train Loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] | Train Loss: 0.4154\n",
      "Epoch [16/20] | Val Loss: 0.5232 | Val Acc: 0.6877 | Val Word Acc: 0.3274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 17 : 100%|████████████████████████████| 1066/1066 [00:20<00:00, 51.35it/s, Train Loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] | Train Loss: 0.4012\n",
      "Epoch [17/20] | Val Loss: 0.5114 | Val Acc: 0.6944 | Val Word Acc: 0.3430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 18 : 100%|████████████████████████████| 1066/1066 [00:21<00:00, 50.72it/s, Train Loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] | Train Loss: 0.3888\n",
      "Epoch [18/20] | Val Loss: 0.5191 | Val Acc: 0.6885 | Val Word Acc: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 19 : 100%|████████████████████████████| 1066/1066 [00:21<00:00, 50.68it/s, Train Loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] | Train Loss: 0.3762\n",
      "Epoch [19/20] | Val Loss: 0.4945 | Val Acc: 0.7156 | Val Word Acc: 0.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 20 : 100%|████████████████████████████| 1066/1066 [00:23<00:00, 44.72it/s, Train Loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] | Train Loss: 0.3655\n",
      "Epoch [20/20] | Val Loss: 0.4906 | Val Acc: 0.7097 | Val Word Acc: 0.3627\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, train_dataset.max_dec_seq_len,embedding_dim=50,hidden_size_enc=64, hidden_size_dec=64, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce92ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "    break\n",
    "\n",
    "torch.set_printoptions(threshold=10000, linewidth=1000)\n",
    "DEC_OUT[2].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEC_CHK = model.predict_greedy(batch)\n",
    "DEC_CHK[2].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, desc=f\"Epoch : {epoch} \")\n",
    "        for (encoder_input, decoder_input, decoder_target) in tqdm_loader:\n",
    "            # Move data to the appropriate device\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_target = decoder_target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            hidden = model.encoder(encoder_input)\n",
    "            decoder_output, _ = model.decoder(decoder_input, hidden)\n",
    "\n",
    "            # Reshape output and target for loss calculation\n",
    "            batch_size, dec_seq_len, vocab_size = decoder_output.shape\n",
    "            decoder_output = decoder_output.view(-1, vocab_size)  # (batch_size * seq_len, vocab)\n",
    "            decoder_target = decoder_target.view(-1, vocab_size).argmax(dim=1)  # class indices\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            tqdm_loader.set_postfix({\n",
    "            \"Train loss (batch)\" : loss.item(),\n",
    "            })\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_acc = validate_seq2seq(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] | Val Loss: {val_loss:.4f} | Val Acc : {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "def validate_seq2seq(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_input, _, decoder_target in val_loader:\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_target = decoder_target.to(device)\n",
    "\n",
    "            # ---- 1. Loss calculation (using teacher forcing only for loss) ----\n",
    "            hidden = model.encoder(encoder_input)\n",
    "            batch_size, dec_seq_len, vocab_size = decoder_target.shape\n",
    "\n",
    "            # Prepare decoder input using start tokens\n",
    "            decoder_input = torch.zeros(batch_size, dec_seq_len, vocab_size).to(device)\n",
    "            start_token_idx = model.output_index_token['\\t']\n",
    "            decoder_input[:, 0, start_token_idx] = 1.0\n",
    "\n",
    "            # Fill decoder input with shifted decoder_target (teacher forcing)\n",
    "            decoder_input[:, 1:] = decoder_target[:, :-1]\n",
    "\n",
    "            outputs, _ = model.decoder(decoder_input, hidden)\n",
    "            outputs_flat = outputs.view(-1, vocab_size)\n",
    "            targets_flat = decoder_target.view(-1, vocab_size).argmax(dim=1)\n",
    "            loss = loss_fn(outputs_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \"\"\"\n",
    "            # ---- 2. Accuracy calculation using model.predict() ----\n",
    "            for i in range(encoder_input.size(0)):\n",
    "                # Fix: Unsqueeze to make sure the input has batch size dimension\n",
    "                pred_seq = model.predict(encoder_input[i])  # Make it batch_size=1\n",
    "                true_seq = ''.join([\n",
    "                    model.output_index_token[idx.item()]\n",
    "                    for idx in decoder_target[i].argmax(dim=1)\n",
    "                    if model.output_index_token[idx.item()] not in ['\\t', '\\n']\n",
    "                ])\n",
    "                min_len = min(len(pred_seq), len(true_seq))\n",
    "                correct_chars += sum(pred_seq[j] == true_seq[j] for j in range(min_len))\n",
    "                total_chars += len(true_seq)\n",
    "            \"\"\"\n",
    "\n",
    "            # ---- 2. Accuracy calculation using model.predict() ----\n",
    "            # Fix: Unsqueeze to make sure the input has batch size dimension\n",
    "            pred_seq = model.predict_greedy(encoder_input)  # Make it batch_size=1\n",
    "            pred_max = torch.argmax(pred_seq, dim=2)\n",
    "            val_max = torch.argmax(decoder_target, dim=2)\n",
    "\n",
    "            total_correct = torch.sum(pred_max[val_max!=0] == val_max[val_max!=1])\n",
    "            total_avail = torch.sum(val_max!=1)\n",
    "\n",
    "            correct_chars += total_correct\n",
    "            total_chars += total_avail\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_chars / total_chars if total_chars > 0 else 0.0\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Character Accuracy: {accuracy:.4f}\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a03de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, train_dataset.max_dec_seq_len,64, device).to(device)\n",
    "\n",
    "\n",
    "validate_seq2seq(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34481d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(train_dataset.input_token_index,train_dataset.output_token_index, 512, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_index_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[120][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"123abc\"\n",
    "str2 = \"cde456\"\n",
    "list1 = []\n",
    "list1.append(str1)\n",
    "list1.append(str2)\n",
    "\n",
    "sorted(set(\" \".join(list1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3944aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[\"hello\",5],[\"hel\",3]], columns=[\"A\",\"B\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55edb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    return \"\\t\" + str + \"\\n\"\n",
    "\n",
    "df[\"A\"] = df[\"A\"].apply(f)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
