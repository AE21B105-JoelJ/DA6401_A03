{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620fc1d6",
   "metadata": {},
   "source": [
    "# Attention Seq2Seq Report Tester Notebook\n",
    "This notebook is used to take the best model from the sweep retrain the model using appropriate callbacks and then predict on the test set and save it and also create some visualizations if required. Without much details lets get into the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193dd313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/joel/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mae21b105\u001b[0m (\u001b[33mRough\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries #\n",
    "# Importing the necessary libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as Fn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.login(key = \"5ef7c4bbfa350a2ffd3c198cb9289f544e3a0910\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39753f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Loading the dataset\n",
    "df_train = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.train.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_test = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.test.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])\n",
    "df_val = pd.read_csv('/home/joel/DA6401_DL/DA6401_A03/ta_lexicons/ta.translit.sampled.dev.tsv', sep='\\t',  header=None, names=[\"native\",\"latin\",\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset for the model to fit #\n",
    "class Dataset_Tamil(Dataset):\n",
    "    def __init__(self, dataframe, build_vocab=True, input_token_index=None, output_token_index=None,\n",
    "                 max_enc_seq_len=0, max_dec_seq_len=0):\n",
    "        \n",
    "        # Input variables\n",
    "        self.input_df = dataframe\n",
    "        self.input_words = []\n",
    "        self.output_words = []\n",
    "        # Characters of the language\n",
    "        self.input_characters = set()\n",
    "        self.output_characters = set()\n",
    "\n",
    "        # Iterating thorough the rows\n",
    "        for _, row in self.input_df.iterrows():\n",
    "            input_word = str(row[\"latin\"])\n",
    "            output_word = \"\\t\" + str(row[\"native\"]) + \"\\n\"\n",
    "            self.input_words.append(input_word)\n",
    "            self.output_words.append(output_word)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.build_vocab()\n",
    "        else:\n",
    "            # Token index for sequence building\n",
    "            self.input_token_index = input_token_index\n",
    "            self.output_token_index = output_token_index\n",
    "            # Heuristics lengths for the encoder decoder\n",
    "            self.max_enc_seq_len = max_enc_seq_len\n",
    "            self.max_dec_seq_len = max_dec_seq_len\n",
    "\n",
    "        # Finding the encoder/decoder tokens \n",
    "        self.total_encoder_tokens = len(self.input_token_index)\n",
    "        self.total_decoder_tokens = len(self.output_token_index)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Building the vocabulary\n",
    "        self.input_characters = sorted(set(\" \".join(self.input_words)))\n",
    "        self.output_characters = sorted(set(\" \".join(self.output_words)))\n",
    "        # Adding the padding character if not present\n",
    "        if \" \" not in self.input_characters:\n",
    "            self.input_characters.append(\" \")\n",
    "        if \" \" not in self.output_characters:\n",
    "            self.output_characters.append(\" \")\n",
    "\n",
    "        # Fitting/Finding the necessary values from training data\n",
    "        self.input_token_index = {char: i for i, char in enumerate(self.input_characters)}\n",
    "        self.output_token_index = {char: i for i, char in enumerate(self.output_characters)}\n",
    "\n",
    "        self.input_token_index_reversed = {i: char for i, char in enumerate(self.input_characters)}\n",
    "        self.output_token_index_reversed = {i: char for i, char in enumerate(self.output_characters)}\n",
    "\n",
    "        self.max_enc_seq_len = max(len(txt) for txt in self.input_words)\n",
    "        self.max_dec_seq_len = max(len(txt) for txt in self.output_words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_words)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_word = self.input_words[index]\n",
    "        output_word = self.output_words[index]\n",
    "\n",
    "        # Finding the input for each stages of the network\n",
    "        encoder_input = np.zeros((self.max_enc_seq_len, self.total_encoder_tokens), dtype=np.float32)\n",
    "        decoder_input = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "        decoder_output = np.zeros((self.max_dec_seq_len, self.total_decoder_tokens), dtype=np.float32)\n",
    "\n",
    "        for t, char in enumerate(input_word):\n",
    "            if char in self.input_token_index:\n",
    "                encoder_input[t, self.input_token_index[char]] = 1.0\n",
    "        for t in range(len(input_word), self.max_enc_seq_len):\n",
    "            encoder_input[t, self.input_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t, char in enumerate(output_word):\n",
    "            if char in self.output_token_index:\n",
    "                decoder_input[t, self.output_token_index[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    decoder_output[t - 1, self.output_token_index[char]] = 1.0\n",
    "        # Fill remaining positions with space character\n",
    "        for t in range(len(output_word), self.max_dec_seq_len):\n",
    "            decoder_input[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t in range(len(output_word) - 1, self.max_dec_seq_len):\n",
    "            decoder_output[t, self.output_token_index[\" \"]] = 1.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(encoder_input),\n",
    "            torch.from_numpy(decoder_input),\n",
    "            torch.from_numpy(decoder_output)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa433590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes definitions #\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type=\"RNN\", num_layers=1, bi_directional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.enc = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.enc = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "        else:\n",
    "            self.enc = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers, bidirectional=bi_directional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden, (hn, cn) = self.enc(x)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.enc(x)\n",
    "            return hidden, out\n",
    "        \n",
    "class Attention_Mechanism(nn.Module):\n",
    "    def __init__(self, hidden_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Creating the matrices for attention calculation\n",
    "        self.V_att = nn.Parameter(torch.randn(size=(self.hidden_dim, 1), device=device)*0.1)\n",
    "        self.U_att = nn.Parameter(torch.randn(size=(self.hidden_dim, self.hidden_dim), device=device)*0.1)\n",
    "        self.W_att = nn.Parameter(torch.randn(size=(self.hidden_dim, self.hidden_dim), device=device)*0.1)\n",
    "\n",
    "    def forward(self, st_1, c_j, mask):\n",
    "        # Compute the attention scores and softmax\n",
    "        \"\"\"\n",
    "        st_1 : input of size (bx1xd)\n",
    "        c_j : input of size (bxLxd)\n",
    "        \"\"\"\n",
    "        #print(st_1.shape, c_j.shape)\n",
    "        inside = self.tanh(torch.matmul(c_j, self.W_att) + torch.matmul(st_1, self.U_att))\n",
    "        #print(inside.shape)\n",
    "        scores = torch.matmul(inside, self.V_att).squeeze(2)\n",
    "        #print(scores.shape)\n",
    "        scores[mask] = -torch.inf\n",
    "\n",
    "        attention = self.softmax(scores)\n",
    "        return attention\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.3, cell_type='RNN', num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.cell_type = cell_type.upper()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.dec = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.dec = nn.GRU(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "        else:\n",
    "            self.dec = nn.RNN(input_size, hidden_size, batch_first=True, dropout=self.dropout, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if states == None:\n",
    "            hidden, out = self.dec(x)\n",
    "            return hidden, out\n",
    "        elif type(states) == tuple:\n",
    "            hidden, (hn, cn) = self.dec(x, states)\n",
    "            return hidden, (hn, cn)\n",
    "        else:\n",
    "            hidden, out = self.dec(x, states)\n",
    "            return hidden, out\n",
    "        \n",
    "class Attention_Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_token_index, output_token_index, max_dec_seq_len, embedding_dim,hidden_size_enc, bi_directional=False,\n",
    "            nature=\"train\", enc_cell=\"LSTM\", dec_cell=\"LSTM\", num_layers=1,dropout=0.2, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_index_token = input_token_index\n",
    "        self.output_index_token = output_token_index\n",
    "        self.max_dec_seq_len = max_dec_seq_len\n",
    "        self.nature = nature\n",
    "        self.enc_cell_type = enc_cell.upper()\n",
    "        self.dec_cell_type = dec_cell.upper()\n",
    "        self.num_layers= num_layers\n",
    "        self.bi_directional = bi_directional\n",
    "        self.hidden_size_enc = hidden_size_enc\n",
    "        self.hidden_size_dec = (1 + int(self.bi_directional == True))*hidden_size_enc\n",
    "        self.embedding = nn.Linear(in_features=len(self.input_index_token), out_features=embedding_dim)\n",
    "        self.embedding_act = nn.Tanh()\n",
    "        self.encoder = Encoder(input_size=embedding_dim, hidden_size=hidden_size_enc, dropout=dropout, cell_type=enc_cell, num_layers=num_layers, bi_directional=self.bi_directional).to(device)\n",
    "        self.attention = Attention_Mechanism(hidden_dim=self.hidden_size_dec)\n",
    "        self.decoder = Decoder(input_size=len(self.output_index_token)+self.hidden_size_dec, hidden_size=self.hidden_size_dec, dropout=dropout, cell_type=dec_cell, num_layers=num_layers).to(device)\n",
    "        self.device = device\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size_dec, out_features=len(output_token_index))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        mask_ = torch.argmax(ENC_IN, 2) == 2\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, self.max_dec_seq_len, len(self.output_index_token), device=self.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = DEC_IN[:, 0:1, :].clone()\n",
    "        for t in range(self.max_dec_seq_len):\n",
    "            if t==0:\n",
    "                out_step, states_dec = self.decoder(torch.cat((in_, hidden_enc[:,-1,:].unsqueeze(1)), dim=2), None)  \n",
    "            else:\n",
    "                # input for next input\n",
    "                in_ = DEC_IN[:, t, :].unsqueeze(1).clone()\n",
    "                att_scores = self.attention(out_step, hidden_enc, mask_)\n",
    "\n",
    "                in_ = torch.cat((in_, torch.bmm(att_scores.unsqueeze(1), hidden_enc)), dim=2)\n",
    "                # Output\n",
    "                out_step, states_dec = self.decoder(in_, states_dec) \n",
    "\n",
    "            logits_step = self.fc(out_step.squeeze(1))         \n",
    "            final_out[:, t, :] = logits_step\n",
    "   \n",
    "        return final_out\n",
    "    \n",
    "    def predict_greedy(self, batch):\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(self.device)\n",
    "        DEC_IN = DEC_IN.to(self.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = self.embedding_act(self.embedding(ENC_IN))\n",
    "        mask_ = torch.argmax(ENC_IN, 2) == 2\n",
    "        hidden_enc, states_enc = self.encoder(input_embedding)\n",
    "\n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, self.max_dec_seq_len, len(self.output_index_token), device=self.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "        in_[:, 0, 0] = 1.0\n",
    "\n",
    "        for t in range(self.max_dec_seq_len):\n",
    "            if t==0:\n",
    "                out_step, states_dec = self.decoder(torch.cat((in_, hidden_enc[:,-1,:].unsqueeze(1)), dim=2), None)  \n",
    "            else:\n",
    "                out_step, states_dec = self.decoder(in_, states_dec)  \n",
    "\n",
    "            logits_step = self.fc(out_step.squeeze(1))            \n",
    "            final_out[:, t, :] = logits_step\n",
    "\n",
    "            # Greedy argmax for next input\n",
    "            top1 = torch.argmax(logits_step, dim=1)              \n",
    "            in_ = torch.zeros(batch_size, 1, len(self.output_index_token), device=self.device)\n",
    "            in_[torch.arange(batch_size), 0, top1] = 1.0\n",
    "            att_scores = self.attention(out_step, hidden_enc, mask_)\n",
    "\n",
    "            in_ = torch.cat((in_, torch.bmm(att_scores.unsqueeze(1), hidden_enc)), dim=2)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09107525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion for validation of the model # \n",
    "def validate_seq2seq(model, val_loader, device, val_type = \"greedy\", beam_width=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_words = 0\n",
    "    total_words = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_progress = tqdm(val_loader, desc=\"Predicting...\")\n",
    "        for batch in tqdm_progress:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            vocab_size = decoder_output.size(-1)\n",
    "            decoder_output = decoder_output.view(-1, vocab_size)\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Character-wise accuracy\n",
    "            if val_type == \"greedy\":\n",
    "                decoder_output = model.predict_greedy(batch)\n",
    "            else:\n",
    "                decoder_output = model.predict_beam_search(batch, beam_width=beam_width)\n",
    "\n",
    "            #print(decoder_output.shape)\n",
    "            pred_tokens = decoder_output.argmax(dim=2)\n",
    "            true_tokens = DEC_OUT.argmax(dim=2)\n",
    "            #print(pred_tokens.shape)\n",
    "            #print(true_tokens.shape)\n",
    "            \n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            correct_chars += (pred_tokens[mask] == true_tokens[mask]).sum().item()\n",
    "            total_chars += mask.sum().item()\n",
    "\n",
    "            mask = true_tokens != 2  # Ignore PAD tokens\n",
    "            #print(mask.shape)\n",
    "            total_words += decoder_output.shape[0]\n",
    "            #print(pred_tokens[mask].shape)\n",
    "            chk_words = (mask.int() - (pred_tokens == true_tokens).int())\n",
    "            chk_words[mask == False] = 0\n",
    "            correct_words += (chk_words.sum(dim = 1) == 0).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_chars / total_chars if total_chars > 0 else 0.0\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0.0\n",
    "    return avg_loss, accuracy, word_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainloop\n",
    "def train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs, device, beam_sizes = [3,5], run=None):\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=2)  # 2 is the padding index\n",
    "    max_val_char_acc = 0\n",
    "    max_val_word_acc = 0\n",
    "    print(\"Training of the model has started...\")\n",
    "    counter = 0\n",
    "    patience = 7\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        tqdm_loader = tqdm(train_loader, desc=f\"Epoch : {epoch + 1} \", ncols=100)\n",
    "\n",
    "        for batch in tqdm_loader:\n",
    "            ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "            ENC_IN = ENC_IN.to(device)\n",
    "            DEC_IN = DEC_IN.to(device)\n",
    "            DEC_OUT = DEC_OUT.to(device)\n",
    "            # Move to device\n",
    "            decoder_output = model(batch)\n",
    "\n",
    "            # Reshape for loss\n",
    "            decoder_output = decoder_output.view(-1, decoder_output.size(-1))\n",
    "            decoder_target_indices = DEC_OUT.argmax(dim=-1).view(-1)\n",
    "\n",
    "            loss = loss_fn(decoder_output, decoder_target_indices)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            tqdm_loader.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_acc, val_word_acc = validate_seq2seq(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Word Acc: {val_word_acc:.4f}\")\n",
    "\n",
    "        if run is not None:\n",
    "            run.log({\"train_loss_epoch\" : avg_loss, \"val_loss_epoch\" : val_loss, \"val_char_acc\" : val_acc, \"val_word_acc\" : val_word_acc})\n",
    "\n",
    "        if val_word_acc > max_val_word_acc:\n",
    "            max_val_char_acc = val_acc\n",
    "            max_val_word_acc = val_word_acc\n",
    "            torch.save(model.state_dict(),\"Attention_weights.pth\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter > patience:\n",
    "            break\n",
    "\n",
    "    if run is not None:\n",
    "        run.summary[\"max_val_char_acc\"] = max_val_char_acc\n",
    "        run.summary[\"max_val_word_acc\"] = max_val_word_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce3865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel/Pytorch_CUDA/virt_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "config = {\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"dropout_rnn\" : 0.2, \n",
    "        \"batch_size\" :  256,\n",
    "        \"epochs\" : 30,\n",
    "        \"embedding_dim\" : 256,\n",
    "        \"num_layers\" : 1,\n",
    "        \"hidden_size_enc\" : 128,\n",
    "        \"enc_cell_type\" : \"GRU\",\n",
    "        \"dec_cell_type\" : \"RNN\",\n",
    "        \"bi_directional\" : True,\n",
    "    }\n",
    "\n",
    "# Loading the datasets and dataloaders\n",
    "train_dataset = Dataset_Tamil(df_train)\n",
    "val_dataset = Dataset_Tamil(df_val, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "test_dataset = Dataset_Tamil(df_test, build_vocab=False, input_token_index=train_dataset.input_token_index, \n",
    "                            output_token_index=train_dataset.output_token_index, max_enc_seq_len=train_dataset.max_enc_seq_len,\n",
    "                            max_dec_seq_len=train_dataset.max_dec_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Attention_Seq2Seq(input_token_index=train_dataset.input_token_index, output_token_index=train_dataset.output_token_index, max_dec_seq_len=train_dataset.max_dec_seq_len,\n",
    "                embedding_dim=config[\"embedding_dim\"], hidden_size_enc=config[\"hidden_size_enc\"], bi_directional=config[\"bi_directional\"], enc_cell=config[\"enc_cell_type\"], dec_cell=config[\"dec_cell_type\"], \n",
    "                num_layers=config[\"num_layers\"], dropout=config[\"dropout_rnn\"], device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "#train_seq2seq(model, train_loader, val_loader, optimizer, num_epochs=config[\"epochs\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5493153b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best = Attention_Seq2Seq(input_token_index=train_dataset.input_token_index, output_token_index=train_dataset.output_token_index, max_dec_seq_len=train_dataset.max_dec_seq_len,\n",
    "                embedding_dim=config[\"embedding_dim\"], hidden_size_enc=config[\"hidden_size_enc\"], bi_directional=config[\"bi_directional\"], enc_cell=config[\"enc_cell_type\"], dec_cell=config[\"dec_cell_type\"], \n",
    "                num_layers=config[\"num_layers\"], dropout=config[\"dropout_rnn\"], device=device).to(device)\n",
    "\n",
    "model_best.load_state_dict(torch.load(\"/home/joel/DA6401_DL/DA6401_A03/Attention_weights.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17108d67-c28d-479e-9c11-0f5649cfbb56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T23:17:07.382893Z",
     "iopub.status.busy": "2025-05-19T23:17:07.382129Z",
     "iopub.status.idle": "2025-05-19T23:17:14.010949Z",
     "shell.execute_reply": "2025-05-19T23:17:14.010344Z",
     "shell.execute_reply.started": "2025-05-19T23:17:07.382868Z"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(entity=\"A3_DA6401_DL\", project=\"Attention_RNN\", name=\"Attention weights final chk\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529820c-3e6e-48fa-95b2-db3fcb22086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wwights\n",
    "import plotly.graph_objects as go\n",
    "def log_attention_heatmap(attention, input_tokens, output_tokens, step, name=\"Attention Heatmap\", i=0):\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention,\n",
    "        colorscale='Blues',\n",
    "        zmin=0,\n",
    "        zmax=1\n",
    "    ))\n",
    "\n",
    "    fig.update_xaxes(tickmode='array', tickvals=list(range(len(input_tokens))), ticktext=input_tokens, constrain='domain')\n",
    "    fig.update_yaxes(tickmode='array', tickvals=list(range(len(output_tokens))), ticktext=output_tokens, scaleanchor=\"x\", scaleratio=1, constrain='domain')\n",
    "\n",
    "    fig.update_layout(xaxis_title=\"Input Tokens\", yaxis_title=\"Output Tokens\", autosize=True, margin=dict(l=50, r=50, t=50, b=50), width=500, height=500, \n",
    "        title=dict(text=f'{name} [in : {\"\".join(input_tokens)}] [out : {\"\".join(output_tokens)}]', x=0.5, font=dict(size=12)))\n",
    "    fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b59922-fc00-478a-bec5-a9958781031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL = [20, 30, 430, 2040, 2555, 3495, 4295, 5875, 6275, 4141, 3254] # Handpicked random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b8e9b-fa93-4b2f-99ab-70c0f85a9251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T23:20:59.902048Z",
     "iopub.status.busy": "2025-05-19T23:20:59.901350Z",
     "iopub.status.idle": "2025-05-19T23:21:00.502652Z",
     "shell.execute_reply": "2025-05-19T23:21:00.501959Z",
     "shell.execute_reply.started": "2025-05-19T23:20:59.902022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test data attention weights finder\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "i = 0\n",
    "X = 0\n",
    "FIGS = []\n",
    "for batch in test_loader:\n",
    "    INPUT_TOKEN = []\n",
    "    OUTPUT_TOKEN = []\n",
    "    ATT_WEIGHTS = np.zeros((train_dataset.max_dec_seq_len, train_dataset.max_enc_seq_len))\n",
    "    if i == TOTAL[X]:\n",
    "        i += 1\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(model_best.device)\n",
    "        DEC_IN = DEC_IN.to(model_best.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = model_best.embedding_act(model_best.embedding(ENC_IN))\n",
    "        mask_ = torch.argmax(ENC_IN, 2) == 2\n",
    "        hidden_enc, states_enc = model_best.encoder(input_embedding)\n",
    "\n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, model_best.max_dec_seq_len, len(model_best.output_index_token), device=model_best.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = torch.zeros(batch_size, 1, len(model_best.output_index_token), device=model_best.device)\n",
    "        in_[:, 0, 0] = 1.0\n",
    "\n",
    "        for t in range(model_best.max_dec_seq_len):\n",
    "            if t==0:\n",
    "                out_step, states_dec = model_best.decoder(torch.cat((in_, hidden_enc[:,-1,:].unsqueeze(1)), dim=2), None) \n",
    "            else:\n",
    "                out_step, states_dec = model_best.decoder(in_, states_dec)\n",
    "\n",
    "            logits_step = model_best.fc(out_step.squeeze(1))   \n",
    "            final_out[:, t, :] = logits_step\n",
    "\n",
    "            # Greedy argmax for next input\n",
    "            top1 = torch.argmax(logits_step, dim=1)          \n",
    "            in_ = torch.zeros(batch_size, 1, len(model_best.output_index_token), device=model_best.device)\n",
    "            in_[torch.arange(batch_size), 0, top1] = 1.0\n",
    "            att_scores = model_best.attention(out_step, hidden_enc, mask_)\n",
    "\n",
    "            in_ = torch.cat((in_, torch.bmm(att_scores.unsqueeze(1), hidden_enc)), dim=2)\n",
    "\n",
    "            ATT_WEIGHTS[t, :] = att_scores.detach().cpu().numpy()\n",
    "    else:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    input_word_vec = ENC_IN[0].argmax(1)\n",
    "    output_pred_vec = final_out[0].argmax(1)\n",
    "    \n",
    "    for jx in range(train_dataset.max_dec_seq_len):\n",
    "        char = train_dataset.output_token_index_reversed[output_pred_vec[jx].item()]\n",
    "        if char == \"\\n\":\n",
    "            break\n",
    "        OUTPUT_TOKEN.append(char)      \n",
    "    \n",
    "    for jx in range(train_dataset.max_enc_seq_len):\n",
    "        char = train_dataset.input_token_index_reversed[input_word_vec[jx].item()]\n",
    "        if char == \" \":\n",
    "            break\n",
    "        INPUT_TOKEN.append(char) \n",
    "    \n",
    "    FINAL_ATT = ATT_WEIGHTS[:len(OUTPUT_TOKEN), :len(INPUT_TOKEN)]\n",
    "    TOTAL_ATT = ATT_WEIGHTS.copy()\n",
    "    \n",
    "    FINAL_ATT.shape\n",
    "    FIG = log_attention_heatmap(FINAL_ATT, INPUT_TOKEN, OUTPUT_TOKEN, step=1, name=\"Attention Weights Heatmap\", i=i)\n",
    "    FIGS.append(FIG)\n",
    "\n",
    "    break\n",
    "\n",
    "print(INPUT_TOKEN)\n",
    "print(OUTPUT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d38ecf-9616-4cd7-95f8-891366af1ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T23:20:27.121374Z",
     "iopub.status.busy": "2025-05-19T23:20:27.120785Z",
     "iopub.status.idle": "2025-05-19T23:20:29.178705Z",
     "shell.execute_reply": "2025-05-19T23:20:29.178204Z",
     "shell.execute_reply.started": "2025-05-19T23:20:27.121351Z"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cf91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'k', 'a', 'l', 'v', 'a', 'a', 'i', 'v', 'i', 'l']\n",
      "['அ', 'க', 'ழ', '்', 'வ', 'ா', 'ய', '்', 'வ', 'ி', 'ல', '்']\n",
      "(28, 30)\n"
     ]
    }
   ],
   "source": [
    "# Test data for connectivity\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "i = 0\n",
    "X = 1\n",
    "FIGS = []\n",
    "for batch in test_loader:\n",
    "    INPUT_TOKEN = []\n",
    "    OUTPUT_TOKEN = []\n",
    "    GRADS_final = np.zeros((train_dataset.max_dec_seq_len, train_dataset.max_enc_seq_len))\n",
    "    if i == TOTAL[X]:\n",
    "        i += 1\n",
    "        ENC_IN, DEC_IN, DEC_OUT = batch\n",
    "        ENC_IN = ENC_IN.to(model_best.device)\n",
    "        DEC_IN = DEC_IN.to(model_best.device)\n",
    "\n",
    "        batch_size = ENC_IN.size(0)\n",
    "        input_embedding = model_best.embedding_act(model_best.embedding(ENC_IN))\n",
    "        mask_ = torch.argmax(ENC_IN, 2) == 2\n",
    "        hidden_enc, states_enc = model_best.encoder(input_embedding)\n",
    "\n",
    "        # Final matrix\n",
    "        final_out = torch.zeros(batch_size, model_best.max_dec_seq_len, len(model_best.output_index_token), device=model_best.device)\n",
    "\n",
    "        # Initial decoder input (with start token)\n",
    "        in_ = torch.zeros(batch_size, 1, len(model_best.output_index_token), device=model_best.device)\n",
    "        in_[:, 0, 0] = 1.0\n",
    "\n",
    "        for t in range(model_best.max_dec_seq_len):\n",
    "            if t==0:\n",
    "                out_step, states_dec = model_best.decoder(torch.cat((in_, hidden_enc[:,-1,:].unsqueeze(1)), dim=2), None) \n",
    "            else:\n",
    "                out_step, states_dec = model_best.decoder(in_, states_dec)  \n",
    "\n",
    "            logits_step = model_best.fc(out_step.squeeze(1))          \n",
    "            final_out[:, t, :] = logits_step\n",
    "\n",
    "            grads_ = torch.autograd.grad(outputs=logits_step.sum(), inputs=input_embedding, retain_graph=True)[0]\n",
    "            squeezed_grads = grads_.squeeze(0).norm(dim=1).unsqueeze(0).detach().cpu().numpy()\n",
    "            GRADS_final[t, :] = squeezed_grads\n",
    "\n",
    "            # Greedy argmax for next input\n",
    "            top1 = torch.argmax(logits_step, dim=1)              \n",
    "            in_ = torch.zeros(batch_size, 1, len(model_best.output_index_token), device=model_best.device)\n",
    "            in_[torch.arange(batch_size), 0, top1] = 1.0\n",
    "            att_scores = model_best.attention(out_step, hidden_enc, mask_)\n",
    "\n",
    "            in_ = torch.cat((in_, torch.bmm(att_scores.unsqueeze(1), hidden_enc)), dim=2)\n",
    "\n",
    "    else:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    input_word_vec = ENC_IN[0].argmax(1)\n",
    "    output_pred_vec = final_out[0].argmax(1)\n",
    "    \n",
    "    for jx in range(train_dataset.max_dec_seq_len):\n",
    "        char = train_dataset.output_token_index_reversed[output_pred_vec[jx].item()]\n",
    "        if char == \"\\n\":\n",
    "            break\n",
    "        OUTPUT_TOKEN.append(char)      \n",
    "    \n",
    "    for jx in range(train_dataset.max_enc_seq_len):\n",
    "        char = train_dataset.input_token_index_reversed[input_word_vec[jx].item()]\n",
    "        if char == \" \":\n",
    "            break\n",
    "        INPUT_TOKEN.append(char) \n",
    "\n",
    "    break\n",
    "\n",
    "print(INPUT_TOKEN)\n",
    "print(OUTPUT_TOKEN)\n",
    "print(GRADS_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e40db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def get_gradient_norms(grad_list, word_in, word_out):\n",
    "    grad_list = grad_list[:len(word_in), :len(word_out)].copy()\n",
    "    norms_per_step = np.zeros_like(grad_list)\n",
    "    for i in range(grad_list.shape[0]):\n",
    "        scaled = MinMaxScaler().fit_transform(grad_list[i].reshape(-1, 1)).flatten()\n",
    "        scaled_norm = scaled / scaled.sum()\n",
    "        norms_per_step[i] = scaled_norm\n",
    "\n",
    "    return norms_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b2412fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7531229 , 0.16131324, 0.02733328, 0.00931772, 0.00210579,\n",
       "        0.        , 0.00227567, 0.03703563, 0.00749577],\n",
       "       [0.06023386, 0.04957183, 0.29597427, 0.13271008, 0.15445384,\n",
       "        0.15883172, 0.10249771, 0.04572667, 0.        ],\n",
       "       [0.06979467, 0.03745478, 0.15508828, 0.21106213, 0.23021964,\n",
       "        0.20188429, 0.05587937, 0.03861683, 0.        ],\n",
       "       [0.06388663, 0.0694043 , 0.10765719, 0.22707103, 0.20969904,\n",
       "        0.20485047, 0.06111545, 0.0563159 , 0.        ],\n",
       "       [0.09472936, 0.06320268, 0.12440794, 0.14267857, 0.18454592,\n",
       "        0.20027521, 0.10644718, 0.08371314, 0.        ],\n",
       "       [0.0311924 , 0.02207705, 0.03607198, 0.09640151, 0.14684419,\n",
       "        0.1460138 , 0.22866576, 0.29273331, 0.        ],\n",
       "       [0.05429193, 0.        , 0.03109674, 0.10606389, 0.17904135,\n",
       "        0.13401896, 0.19229774, 0.28943444, 0.01375496]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_norms(GRADS_final, INPUT_TOKEN, OUTPUT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b11247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_connectivity_stacked(GRADS_final, INPUT_TOKEN, OUTPUT_TOKEN):\n",
    "    max_dec_len, max_enc_len = GRADS_final.shape\n",
    "\n",
    "    # Ensure input and output tokens are padded\n",
    "    input_chars = INPUT_TOKEN + [''] * (max_enc_len - len(INPUT_TOKEN))\n",
    "    output_labels = OUTPUT_TOKEN + [''] * (max_dec_len - len(OUTPUT_TOKEN))\n",
    "\n",
    "    # Create full matrix for plotting (each row = output token step)\n",
    "    full_matrix = GRADS_final[:len(output_labels), :len(input_chars)]\n",
    "\n",
    "    # Repeat input_chars for each row to place inside boxes\n",
    "    repeated_text = np.tile(input_chars, (len(output_labels), 1))\n",
    "\n",
    "    # Create y-axis labels using output characters\n",
    "    y_labels = [f'{i}: {char}' if char else f'{i}' for i, char in enumerate(output_labels)]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(z=full_matrix, x=list(range(len(input_chars))), y=y_labels, text=repeated_text, texttemplate=\"%{text}\", textfont={\"size\": 14, \"color\": \"black\"},\n",
    "            colorscale='Blues', zmid=0, zmin=GRADS_final.min(), zmax=GRADS_final.max(), showscale=True)\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=f'Gradient Connectivity Matrix [in : {\"\".join(INPUT_TOKEN)}] [out : {\"\".join(OUTPUT_TOKEN)}]', x=0.5, font=dict(size=16)),\n",
    "        xaxis=dict( title=\"Input Characters\", tickmode='array', tickvals=list(range(len(input_chars))), ticktext=input_chars, showgrid=False, scaleanchor='y'),\n",
    "        yaxis=dict(title=\"Output Characters\", tickmode='array', tickvals=list(range(len(output_labels))), ticktext=y_labels, autorange='reversed',showgrid=False,),\n",
    "        width=50 * len(input_chars) + 200,\n",
    "        height=40 * len(output_labels) + 200,\n",
    "        margin=dict(t=40, l=60, r=40, b=40)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22e7d02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"7c951e19-7bda-4cae-b462-4ec37edae3cb\" class=\"plotly-graph-div\" style=\"height:480px; width:650px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"7c951e19-7bda-4cae-b462-4ec37edae3cb\")) {                    Plotly.newPlot(                        \"7c951e19-7bda-4cae-b462-4ec37edae3cb\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"showscale\":true,\"text\":[[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"]],\"textfont\":{\"color\":\"black\",\"size\":14},\"texttemplate\":\"%{text}\",\"x\":[0,1,2,3,4,5,6,7,8],\"y\":[\"0: த\",\"1: ோ\",\"2: ன\",\"3: ்\",\"4: ற\",\"5: ி\",\"6: ய\"],\"z\":{\"dtype\":\"f8\",\"bdata\":\"iFB7NJUZ6D9mYfuA6aXEP9XkT05B\\u002fZs\\u002fOgnM4CsVgz+xpW0TKUBhPwAAAAAAAAAA0iPr0GukYj+O0smMVfaiP4\\u002fwmM\\u002fhs34\\u002fSKsjCPnWrj8SR9i0emGpPztElBI+8dI\\u002fF\\u002fL+16T8wD8Xyqe8JMXDP0LmZQyZVMQ\\u002fWygQSUo9uj9dEQ+YfGmnPwAAAAAAAAAAZmU+SBDesT+m7WfxRS2jP+h5FsXu2cM\\u002fYoGtexUEyz\\u002f85PpR1nfNP1kC2CxY18k\\u002fuDaVlDicrD9fhirrlcWjPwAAAAAAAAAAF8MVv99asD9bbk\\u002fpesSxP01Fr+Zrj7s\\u002fPsa106kQzT9BOf8Ja9fKP2fm\\u002fVSKOMo\\u002f9XiGJYZKrz\\u002f\\u002fAYXxb9WsPwAAAAAAAAAA2kp34i5AuD+FlkMPDS6wP9o5ktcy2b8\\u002ftVx7l0pDwj92ZLtcM5\\u002fHPyjY0UKeosk\\u002f3BRkZx9Auz8+nT1hOW61PwAAAAAAAAAA1cE0pebwnz\\u002fKGrtpXZuWP75ka\\u002fIGeKI\\u002f2PTe58StuD9te6dVysvCP2NBjI6UsMI\\u002fgNxSaOtEzT++i4iAJLzSPwAAAAAAAAAAkfm2ySbMqz8AAAAAAAAAAAhOZhLT158\\u002fc0J4wQAnuz+5CTC30+rGP1PQ4nqIJ8E\\u002fB6sUUzadyD\\u002fGJDUJGIbSP+V\\u002feC+PK4w\\u002f\",\"shape\":\"7, 9\"},\"zmax\":0.7531229043038516,\"zmid\":0,\"zmin\":0.0,\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":16},\"text\":\"Gradient Connectivity Matrix [in : thontriya] [out : தோன்றிய]\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"Input Characters\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6,7,8],\"ticktext\":[\"t\",\"h\",\"o\",\"n\",\"t\",\"r\",\"i\",\"y\",\"a\"],\"showgrid\":false,\"scaleanchor\":\"y\",\"scaleratio\":1},\"yaxis\":{\"title\":{\"text\":\"Output Characters\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6],\"ticktext\":[\"0: த\",\"1: ோ\",\"2: ன\",\"3: ்\",\"4: ற\",\"5: ி\",\"6: ய\"],\"autorange\":\"reversed\",\"showgrid\":false},\"margin\":{\"t\":40,\"l\":60,\"r\":40,\"b\":40},\"width\":650,\"height\":480},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7c951e19-7bda-4cae-b462-4ec37edae3cb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_connectivity_stacked(get_gradient_norms(GRADS_final, INPUT_TOKEN, OUTPUT_TOKEN), INPUT_TOKEN, OUTPUT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7443057,
     "sourceId": 11846133,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7447243,
     "sourceId": 11851914,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
